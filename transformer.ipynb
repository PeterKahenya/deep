{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "Follows the \"Attention is all you need\" paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmeddingsLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.embedding(X) * math.sqrt(self.d_model) # \"In the embedding layers, we multiply those weights by sqrt(d_model)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Ecodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, context_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.pe = torch.zeros(self.context_size, self.d_model,requires_grad=False)\n",
    "        for pos in range(self.context_size):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "                self.pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.d_model)))\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.pe.unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding and position encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024])\n",
      "torch.Size([64, 1024, 512]) tensor([ 37.6751,  27.8283,  -0.8374,  -6.8180,  -5.5830,  52.5307, -13.6899,\n",
      "          5.2778,  -3.5278,   6.6493], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1024, 512]) tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "torch.Size([64, 1024, 512]) tensor([ 37.6751,  28.8283,  -0.8374,  -5.8180,  -5.5830,  53.5307, -13.6899,\n",
      "          6.2778,  -3.5278,   7.6493], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_test = EmeddingsLayer(d_model=512,vocab_size=50000)\n",
    "pencoding = PositionalEncoding(d_model=512,context_size=1024)\n",
    "example_data = torch.randint(1,50000,(64,1024))\n",
    "print(example_data.shape)\n",
    "embed_output = embed_test(example_data)\n",
    "print(embed_output.shape,embed_output[0][0][:10])\n",
    "pe_output = pencoding()\n",
    "print(pe_output.shape,pe_output[0][0][:10])\n",
    "embed_pos_output = embed_output + pe_output\n",
    "print(embed_pos_output.shape,embed_pos_output[0][0][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_dim:int,p_drop:float,masked:bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.masked = masked\n",
    "        self.queries= nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.keys = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.values = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "\n",
    "    def forward(self,Q:torch.Tensor,K:torch.Tensor,V:torch.Tensor) -> torch.Tensor:\n",
    "        B,T,C = K.shape\n",
    "        Q = self.dropout(self.queries(Q))\n",
    "        K = self.dropout(self.keys(K))\n",
    "        V = self.dropout(self.values(V))\n",
    "\n",
    "        scaled_dot_product_attention = (Q @ K.transpose(2,1))/torch.sqrt(torch.tensor(C))\n",
    "\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones(T,T)) == 0\n",
    "            scaled_dot_product_attention = scaled_dot_product_attention.masked_fill(mask,-float(\"inf\"))\n",
    "\n",
    "        dot_product_softened = torch.softmax(scaled_dot_product_attention,dim=-1)\n",
    "        return dot_product_softened @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model:int, p_drop:float, num_heads:int = 8, masked = False) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = math.floor(d_model/num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.heads = [AttentionHead(head_dim=self.head_dim,p_drop=p_drop, masked=masked) for h in range(num_heads)]\n",
    "        self.linear = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "    \n",
    "    def forward(self,X:torch.Tensor,Q:tuple,K:tuple,V:tuple) ->torch.Tensor:\n",
    "        heads_output = []\n",
    "        for head_index,head in enumerate(self.heads):\n",
    "                queries = Q[head_index]\n",
    "                keys = K[head_index]\n",
    "                values = V[head_index]\n",
    "                v = head(queries,keys,values) # this could be distributed to multiple devices for // processing\n",
    "                heads_output.append(v) # accumulate result\n",
    "        \n",
    "        o = torch.cat(heads_output,dim=-1)\n",
    "        linear_output = self.linear(o)\n",
    "        dropped_output = self.dropout(linear_output)\n",
    "        return self.layer_norm(X+dropped_output)\n",
    "\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model=512,p_drop=0.1,num_heads=8,masked=True)\n",
    "sample_data = torch.randn((5,10,512))\n",
    "splits = torch.split(sample_data,64,dim=2)\n",
    "mhsa(sample_data,splits,splits,splits).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model:int,p_drop:float,d_ff:int) -> None:\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model,out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(in_features=d_ff,out_features=d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_norm(X+self.ffn(X))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,p_drop:float, d_ff:int,num_heads:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads)\n",
    "        self.feedforward = FeedForward(d_model=d_model,p_drop=p_drop,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        splits = torch.split(X,self.head_dim,dim=2)\n",
    "        return self.feedforward(self.multihead_self_attention(X,splits,splits,splits))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, number_of_encoder_blocks:int=6,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.encode = nn.Sequential(*[EncoderLayer(**kwargs) for n in range(number_of_encoder_blocks)])\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        return self.encode(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,p_drop:float,d_ff:int,num_heads:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.masked_multi_head_self_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads,masked=True)\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads,masked=False)\n",
    "        self.feedforward = FeedForward(d_model=d_model,p_drop=p_drop,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor) -> torch.Tensor:\n",
    "        output_splits = torch.split(outputs,self.head_dim,dim=2)\n",
    "        encoded_sequence_splits = torch.split(encoded_sequence,self.head_dim,dim=2)\n",
    "        masked_output = self.masked_multi_head_self_attention(outputs,output_splits,output_splits,output_splits)\n",
    "        mhsa_output = self.multi_head_self_attention(masked_output,Q=masked_output,K=encoded_sequence_splits,V=encoded_sequence_splits)\n",
    "        \n",
    "        return self.feedforward(mhsa_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, number_of_decoder_blocks:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(**kwargs) for n in range(number_of_decoder_blocks)])\n",
    "        \n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor) -> torch.Tensor:\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            outputs = decoder_layer(outputs,encoded_sequence)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size:int,\n",
    "                 batch_size:int,\n",
    "                 context_size:int,\n",
    "                 d_model:int,\n",
    "                 d_ff:int,\n",
    "                 num_heads:int,\n",
    "                 number_of_encoder_blocks:int,\n",
    "                 number_of_decoder_blocks:int,\n",
    "                 p_drop:float):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.embedding = EmeddingsLayer(d_model=d_model,vocab_size=vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model,context_size=context_size)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.encoder = Encoder(\n",
    "                            vocab_size=batch_size,\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_encoder_blocks=number_of_encoder_blocks,\n",
    "                            p_drop=p_drop)\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "                            vocab_size=batch_size,\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            p_drop=p_drop,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_decoder_blocks=number_of_decoder_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=d_model,out_features=vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,X:torch.Tensor,y:torch.Tensor) -> torch.Tensor:\n",
    "        pos_encoding = self.positional_encoding()\n",
    "\n",
    "        input_embeddings = self.embedding(X) \n",
    "        inputs = self.dropout(input_embeddings+pos_encoding) # B*T*C\n",
    "        \n",
    "        encoded_sequence = self.encoder(inputs)\n",
    "        output_embedding = self.embedding(y)\n",
    "        outputs = self.dropout(output_embedding+pos_encoding) # B*T*C\n",
    "\n",
    "        decoder_output = self.decoder(outputs,encoded_sequence)\n",
    "\n",
    "        output_logits = self.linear(decoder_output)\n",
    "        output_probs = torch.softmax(output_logits,dim=-1)\n",
    "\n",
    "\n",
    "        return output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30908, 25044, 10012, 22277, 33097, 26681, 15800,   328,  1235, 17931],\n",
      "        [11908, 22090,  2252, 21624, 17150,  8975, 15391, 36947, 10469,  7199],\n",
      "        [23676, 36830, 29603, 15104, 17268, 21152, 31797, 27589, 18469, 12114],\n",
      "        [33004,  3447, 18985, 23113,  4303, 21491, 29902, 24992, 34827, 14415],\n",
      "        [31494, 26420, 19301, 27433, 34343, 31011, 29458, 16520,  3884, 24261],\n",
      "        [35821, 36820, 34717,  9134,  9943, 20816, 10174, 12720, 32344, 30117],\n",
      "        [ 6371, 12510, 26485, 29164, 24968, 16980,  4347, 14114, 13843, 16131],\n",
      "        [ 5055, 13493, 36506,  7870,  3387,  4982,  3830, 14695, 13812,  2677],\n",
      "        [10376, 27180,  3044, 17918, 30099, 31488,  1198, 14573, 34837,  7715],\n",
      "        [17361, 10007, 14353,  7483, 13443, 24416, 21664, 36106, 36532, 14707],\n",
      "        [13519, 23756,  8392, 25098, 26130, 25333, 35014, 26205, 17080, 25167],\n",
      "        [ 3796,  6077,  2738,  8693,  4961, 11041, 24364, 30279, 27484, 11578],\n",
      "        [ 2599,  5368, 33832, 27697, 29990,  7187,   135, 12314,   492, 16012],\n",
      "        [27535, 12679, 28020, 35447, 33893, 25167, 11989, 23690, 35400, 28331],\n",
      "        [20446,  1902, 12939,  7055, 18147, 10438, 12791,  7076, 34842, 36414],\n",
      "        [21152, 35822,  9669,  3573, 33207, 34428, 30010, 35367,  2216, 34080],\n",
      "        [31172, 30046,  7217,  8979, 15529, 25281,  5484, 27387,  2015, 27927],\n",
      "        [27277, 10460, 28717, 14353,   298, 33028, 14990, 14594, 25277,  4554],\n",
      "        [ 6338, 30704, 29578,  8911, 36633, 14270,  5122, 13899, 30386, 23098],\n",
      "        [32073, 26638,  5614, 14217,  2589,  4406, 14235, 21159,  1018, 15284],\n",
      "        [ 4548, 10932, 12421, 34988, 33268,  4013,   668, 25653, 13820, 23579],\n",
      "        [26442, 11211, 30515,  2608, 36553, 36740, 29694, 25246, 11746,  2738],\n",
      "        [27509, 32354,  2986, 27447, 20365,  5310, 15630, 20858, 36020,  3683],\n",
      "        [12980, 19773, 17781, 14997, 10060, 23349, 17774, 22213, 25967, 28635],\n",
      "        [22057,  3859, 17568, 16684,  6839, 20711,  2554,  1100, 34421, 11957],\n",
      "        [17874, 29458, 15724, 22057,  5734, 14064, 22423,  9990,  2349, 33207],\n",
      "        [ 5297,  4091, 17562, 26681,  3331,  2216, 11380,  8178, 28717, 29729],\n",
      "        [ 2713, 32180, 15111, 15111, 12021,  4347, 12406, 22030,  5645,  8385],\n",
      "        [32891,  6369,  4972, 13699, 34466, 33021,  1151, 13048,  6062,  4202],\n",
      "        [16709, 27387,    16, 20043, 20497, 27282, 16684, 36506,  9553,  4845],\n",
      "        [20180, 25529, 11367,  5657, 30856, 34293, 34285, 13271, 27104,  1333],\n",
      "        [20170,   412, 30939,   519, 17568, 18788, 28355, 19444, 21543, 14807],\n",
      "        [36115, 11620, 28559,  8618, 26487, 34792, 24316, 23286, 12527,   499],\n",
      "        [14720, 33494, 25055, 17634, 15658, 28482, 26071,  8123, 17737, 33831],\n",
      "        [12363, 20753, 25494, 23113, 19607, 26367, 17382,  2783, 30370, 19534],\n",
      "        [13071,  2515, 16819,  2216, 21299, 22248, 24319,  6180, 34218, 22057],\n",
      "        [29791,  3158,  1198, 16078, 21699, 27651, 26886,  7118, 33962,  4972],\n",
      "        [23525, 14121, 32680, 19563, 18216, 15821,  7938, 11901, 15669,  8571],\n",
      "        [24631, 24402, 25044, 27209, 29427, 23795, 13911, 18060, 13647,   314],\n",
      "        [ 2976,  9735, 34472, 32520, 26173, 32520,  4202, 12386, 23341, 15662],\n",
      "        [ 1202, 32996, 31071, 16385, 33404,  4447, 16630, 34204, 24038,  5211],\n",
      "        [36830, 13866, 19278, 34204, 20353, 21070, 11049, 20851,  8039, 33738],\n",
      "        [15660, 26269, 32642,  8766,  7710, 17064, 26487, 17200, 36482,  8821],\n",
      "        [29458, 22086, 33929, 26416,  4972,  9340, 15736,  6965, 15058, 30457],\n",
      "        [20326, 18049, 17877, 25234,  1498, 16274,  4708,  5534, 10034, 22386],\n",
      "        [26617, 36740, 16852, 32390, 34233, 19245, 31312, 32084, 18571, 20431],\n",
      "        [28568, 18478, 20858,  3771, 12114, 13132,  3155,  5529,  1488, 21021],\n",
      "        [27576,  5361, 34837, 20914,  8526,  6371, 20858, 16899, 12206,  6855],\n",
      "        [12651,  4692,  4521, 10191,  5477, 28645,  4309,  4848,  3435, 21390],\n",
      "        [20145,  5637, 25809,  6806, 36757, 21589,  5705, 11124, 13647,  2608],\n",
      "        [11249,  1440, 11049,  2088, 26753, 34113, 30791, 28250,  6855,  7197],\n",
      "        [23679, 35397, 27111, 23270,  8442, 31501, 36640,  6557,  5991,  7287],\n",
      "        [14734, 28023, 18491, 33940, 16684, 32599, 32804, 10600, 33217, 19612],\n",
      "        [13307, 19129, 35397, 27629,  8604, 20611, 10825,  7039,  7041,  8266],\n",
      "        [25362, 18924, 36716,  2342, 12114,  4750, 10621, 29583,   187, 19615],\n",
      "        [17043, 16696, 36844, 20897, 25701,  2498, 26019,  2012, 24372, 35400],\n",
      "        [ 2458,  1902,  2475, 17568,  2383,  9944, 17568,  7406, 17615, 16385],\n",
      "        [34952, 31676, 35104, 13556, 27628, 24563, 20949,  4165,  5811,  5434],\n",
      "        [10207, 22891, 12939, 29364,  1395, 30691,  9779, 29619, 23056,   519],\n",
      "        [10241,  6859,  1790,  8266, 32314,  9276, 19299,   301, 31385, 36846],\n",
      "        [10943, 30213, 25031, 11029, 28154,  4712,  9944,  4646,  2208,   713],\n",
      "        [13699, 19767, 32084, 34270, 13274, 30396, 20353, 26394, 21440, 17900],\n",
      "        [17527,  9963, 32779, 12386, 31401,  8717, 15968,  4160, 23588, 36054],\n",
      "        [ 3892, 21097,  1018, 28023, 21597, 35013, 31180, 10273,  4447, 10063]])\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vocab_size\":37000,\n",
    "    \"batch_size\":64,\n",
    "    \"context_size\":10,\n",
    "    \"d_model\":512,\n",
    "    \"num_heads\":1,\n",
    "    \"d_ff\":2048,\n",
    "    \"number_of_encoder_blocks\": 6,\n",
    "    \"number_of_decoder_blocks\": 6,\n",
    "    \"p_drop\":0.1\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Transformer(**config)\n",
    "\n",
    "X = torch.randint(1,config[\"vocab_size\"],(config[\"batch_size\"],config[\"context_size\"]))\n",
    "y = torch.randint(1,config[\"vocab_size\"],(config[\"batch_size\"],config[\"context_size\"]))\n",
    "\n",
    "o = model(X,y)\n",
    "print(o.argmax(dim=-1)) # will substitute for sampling for now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of swahili dataset: 5000 \n",
      "Size of english dataset: 5000 \n",
      "Max swahili sentence: 249 \n",
      "Max english sentence: 233 \n",
      "['Huyo ni rafiki yako mpya?', 'Job hana hamu ya mpira wa vikapu.', 'Adam aliniambia kuwa Alice alikuwa na mpenzi mpya wa kiume', 'Radio haikutanga kuhusu ajali hiyo.', 'Adamu ana wasiwasi tutapotea.']\n",
      "['Is that your new friend?', \"Jacob wasn't interested in baseball.\", 'Adam told me that Alice had a new boyfriend.', \"The radio didn't inform about the accident.\", \"Adam is worried we'll get lost.\"]\n"
     ]
    }
   ],
   "source": [
    "swa_sentences = []\n",
    "with open(\"./data/translate/gamayun_kit5k.swa\",\"r\") as f:\n",
    "    swa_sentences = f.readlines()\n",
    "eng_sentences = []\n",
    "with open(\"./data/translate/gamayun_kit5k.eng\",\"r\") as f:\n",
    "    eng_sentences = f.readlines()\n",
    "\n",
    "swa_sentences = [s.rstrip(\"\\n\") for s in swa_sentences]\n",
    "eng_sentences = [s.rstrip(\"\\n\") for s in eng_sentences]\n",
    "\n",
    "print(f\"Size of swahili dataset: {len(swa_sentences)} \")\n",
    "print(f\"Size of english dataset: {len(eng_sentences)} \")\n",
    "print(f\"Max swahili sentence: {max([len(s) for s in swa_sentences])} \")\n",
    "print(f\"Max english sentence: {max([len(s) for s in eng_sentences])} \")\n",
    "\n",
    "print(swa_sentences[:5])\n",
    "print(eng_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'n', '3', '0', 'o', '7', 'I', 'J', '6', 'l', 'p', 'y', ':', 'f', 'e', 'u', '(', 'm', 'P', 's', 'k', 'U', 'a', 'z', 'F', '/', 'v', 'c', '8', 'D', 'Z', '5', '$', 'O', '&', 'g', 'V', 'Y', ',', '.', 't', 'G', 'B', '1', '?', ';', \"'\", ')', 'S', '-', 'R', 'W', 'A', '!', 'T', 'j', 'i', 'q', 'r', '”', '\"', 'w', '4', 'H', 'C', 'L', 'M', ' ', 'd', 'x', '\\u200b', 'Q', 'b', 'N', 'E', 'K', '9', '+', '—', '2', 'h', '<', '--']\n",
      "['>', 'n', '3', '0', 'o', '’', '7', 'I', 'J', '6', 'l', 'p', 'é', 'y', ':', 'f', 'e', 'u', '(', 'm', 'P', 's', 'k', 'U', 'a', 'z', 'F', 'v', 'c', '8', 'D', '5', 'Z', '$', 'O', '&', 'g', 'V', 'Y', ',', 't', '.', 'G', 'B', '1', '?', ';', \"'\", ')', 'S', '-', 'W', 'R', 'à', 'A', '!', 'T', 'j', 'i', '_', 'q', 'r', '”', '\"', 'w', '4', 'H', 'C', 'L', 'M', ' ', 'd', '°', 'x', 'Q', 'b', '“', 'N', 'E', '9', 'K', '—', '2', 'h', '<', '--']\n",
      "Eng vocab_size :83\n",
      "Swa vocab_size :86\n",
      "{'>': 0, 'n': 1, '3': 2, '0': 3, 'o': 4, '7': 5, 'I': 6, 'J': 7, '6': 8, 'l': 9, 'p': 10, 'y': 11, ':': 12, 'f': 13, 'e': 14, 'u': 15, '(': 16, 'm': 17, 'P': 18, 's': 19, 'k': 20, 'U': 21, 'a': 22, 'z': 23, 'F': 24, '/': 25, 'v': 26, 'c': 27, '8': 28, 'D': 29, 'Z': 30, '5': 31, '$': 32, 'O': 33, '&': 34, 'g': 35, 'V': 36, 'Y': 37, ',': 38, '.': 39, 't': 40, 'G': 41, 'B': 42, '1': 43, '?': 44, ';': 45, \"'\": 46, ')': 47, 'S': 48, '-': 49, 'R': 50, 'W': 51, 'A': 52, '!': 53, 'T': 54, 'j': 55, 'i': 56, 'q': 57, 'r': 58, '”': 59, '\"': 60, 'w': 61, '4': 62, 'H': 63, 'C': 64, 'L': 65, 'M': 66, ' ': 67, 'd': 68, 'x': 69, '\\u200b': 70, 'Q': 71, 'b': 72, 'N': 73, 'E': 74, 'K': 75, '9': 76, '+': 77, '—': 78, '2': 79, 'h': 80, '<': 81, '--': 82}\n",
      "{'>': 0, 'n': 1, '3': 2, '0': 3, 'o': 4, '’': 5, '7': 6, 'I': 7, 'J': 8, '6': 9, 'l': 10, 'p': 11, 'é': 12, 'y': 13, ':': 14, 'f': 15, 'e': 16, 'u': 17, '(': 18, 'm': 19, 'P': 20, 's': 21, 'k': 22, 'U': 23, 'a': 24, 'z': 25, 'F': 26, 'v': 27, 'c': 28, '8': 29, 'D': 30, '5': 31, 'Z': 32, '$': 33, 'O': 34, '&': 35, 'g': 36, 'V': 37, 'Y': 38, ',': 39, 't': 40, '.': 41, 'G': 42, 'B': 43, '1': 44, '?': 45, ';': 46, \"'\": 47, ')': 48, 'S': 49, '-': 50, 'W': 51, 'R': 52, 'à': 53, 'A': 54, '!': 55, 'T': 56, 'j': 57, 'i': 58, '_': 59, 'q': 60, 'r': 61, '”': 62, '\"': 63, 'w': 64, '4': 65, 'H': 66, 'C': 67, 'L': 68, 'M': 69, ' ': 70, 'd': 71, '°': 72, 'x': 73, 'Q': 74, 'b': 75, '“': 76, 'N': 77, 'E': 78, '9': 79, 'K': 80, '—': 81, '2': 82, 'h': 83, '<': 84, '--': 85}\n"
     ]
    }
   ],
   "source": [
    "START_TOKEN = '>'\n",
    "PADDING_TOKEN = '<'\n",
    "END_TOKEN = '--'\n",
    "\n",
    "swa_vocab = list(set(''.join(swa_sentences)))\n",
    "swa_vocab.insert(0,START_TOKEN)\n",
    "swa_vocab.append(PADDING_TOKEN)\n",
    "swa_vocab.append(END_TOKEN)\n",
    "eng_vocab = list(set(''.join(eng_sentences)))\n",
    "eng_vocab.insert(0,START_TOKEN)\n",
    "eng_vocab.append(PADDING_TOKEN)\n",
    "eng_vocab.append(END_TOKEN)\n",
    "\n",
    "print(swa_vocab)\n",
    "print(eng_vocab)\n",
    "\n",
    "print(f\"Eng vocab_size :{len(swa_vocab)}\")\n",
    "print(f\"Swa vocab_size :{len(eng_vocab)}\")\n",
    "\n",
    "swa_token_to_index = {t:i for i,t in enumerate(swa_vocab)}\n",
    "print(swa_token_to_index)\n",
    "swa_index_to_token = {i:t for i,t in enumerate(swa_vocab)}\n",
    "eng_token_to_index = {t:i for i,t in enumerate(eng_vocab)}\n",
    "print(eng_token_to_index)\n",
    "eng_index_to_token = {i:t for i,t in enumerate(eng_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_sentences_tokenized = [[swa_token_to_index[t] for t in s] for s in swa_sentences]\n",
    "english_sentences_tokenized = [[eng_token_to_index[t] for t in s] for s in eng_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "swahili_sentences_tokenized_train = swahili_sentences_tokenized[:4500]\n",
    "swahili_sentences_tokenized_test = swahili_sentences_tokenized[4500:]\n",
    "english_sentences_tokenized_train = english_sentences_tokenized[:4500]\n",
    "english_sentences_tokenized_test = english_sentences_tokenized[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, swahili_sentences, english_sentences,transforms=None):\n",
    "        self.swahili_sentences = swahili_sentences\n",
    "        self.english_sentences = english_sentences\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_sentence = self.english_sentences[idx]\n",
    "        swa_sentence = self.swahili_sentences[idx]\n",
    "\n",
    "        for _ in range(len(eng_sentence), max_sentence_length):\n",
    "            eng_sentence.append(eng_token_to_index[PADDING_TOKEN])\n",
    "        for _ in range(len(swa_sentence), max_sentence_length):\n",
    "            swa_sentence.append(swa_token_to_index[PADDING_TOKEN])\n",
    "\n",
    "        if self.transforms:\n",
    "            swa_sentence = self.transforms(swa_sentence)\n",
    "            eng_sentence = self.transforms(eng_sentence)\n",
    "\n",
    "        # print(eng_sentence.shape,swa_sentence.shape)\n",
    "        return eng_sentence, swa_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 250]) torch.Size([10, 250])\n",
      "tensor([69, 13, 70, 15, 61, 58, 16,  1, 71, 21, 70, 64, 24,  1, 40, 16, 71, 70,\n",
      "        71, 16, 40, 24, 58, 10, 21, 41, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "training_dataset = TranslationDataset(swahili_sentences=swahili_sentences_tokenized_train,english_sentences=english_sentences_tokenized_train,transforms=transforms.ToTensor())\n",
    "testing_dataset = TranslationDataset(swahili_sentences=swahili_sentences_tokenized_test,english_sentences=english_sentences_tokenized_test,transforms=transforms.ToTensor())\n",
    "training_dataloader = DataLoader(training_dataset,batch_size=10,shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_dataset,batch_size=10,shuffle=False)\n",
    "\n",
    "eng_sentence,swa_sentence = next(iter(training_dataloader))\n",
    "print(eng_sentence.shape,swa_sentence.shape)\n",
    "print(eng_sentence[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_size\":37000,\n",
    "    \"batch_size\":64,\n",
    "    \"context_size\":10,\n",
    "    \"d_model\":512,\n",
    "    \"num_heads\":1,\n",
    "    \"d_ff\":2048,\n",
    "    \"number_of_encoder_blocks\": 6,\n",
    "    \"number_of_decoder_blocks\": 6,\n",
    "    \"p_drop\":0.1\n",
    "}\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(eng_batch, kn_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}: {encoder_self_attention_mask[0, :10, :10]}\")\n",
    "    print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}: {decoder_self_attention_mask[0, :10, :10]}\")\n",
    "    print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}: {decoder_cross_attention_mask[0, :10, :10]}\")\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
