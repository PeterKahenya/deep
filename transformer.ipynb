{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "Follows the \"Attention is all you need\" paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmeddingsLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.embedding(X) * math.sqrt(self.d_model) # \"In the embedding layers, we multiply those weights by sqrt(d_model)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Ecodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, context_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.pe = torch.zeros(self.context_size, self.d_model,requires_grad=False)\n",
    "        for pos in range(self.context_size):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "                self.pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.d_model)))\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.pe.unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding and position encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024])\n",
      "torch.Size([64, 1024, 512]) tensor([  8.1030,  23.7976,  56.3750, -24.6229,   4.5044,  23.9931,   0.3400,\n",
      "          5.2031, -21.3926,  55.1794], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1024, 512]) tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "torch.Size([64, 1024, 512]) tensor([  8.1030,  24.7976,  56.3750, -23.6229,   4.5044,  24.9931,   0.3400,\n",
      "          6.2031, -21.3926,  56.1794], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_test = EmeddingsLayer(d_model=512,vocab_size=50000)\n",
    "pencoding = PositionalEncoding(d_model=512,context_size=1024)\n",
    "example_data = torch.randint(1,50000,(64,1024))\n",
    "print(example_data.shape)\n",
    "embed_output = embed_test(example_data)\n",
    "print(embed_output.shape,embed_output[0][0][:10])\n",
    "pe_output = pencoding()\n",
    "print(pe_output.shape,pe_output[0][0][:10])\n",
    "embed_pos_output = embed_output + pe_output\n",
    "print(embed_pos_output.shape,embed_pos_output[0][0][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_dim:int = 64, masked:bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.masked = masked\n",
    "        self.queries= nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.keys = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.values = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "\n",
    "    def forward(self,Q:torch.Tensor,K:torch.Tensor,V:torch.Tensor) -> torch.Tensor:\n",
    "        B,T,C = K.shape\n",
    "        scaled_dot_product_attention = (Q @ K.transpose(2,1))/torch.sqrt(torch.tensor(C))\n",
    "        if self.masked:\n",
    "            mask = torch.tril(torch.ones(T,T)) == 0\n",
    "            scaled_dot_product_attention = scaled_dot_product_attention.masked_fill(mask,-float(\"inf\"))\n",
    "\n",
    "        dot_product_softened = torch.softmax(scaled_dot_product_attention,dim=-1)\n",
    "        return dot_product_softened @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model:int = 512, num_heads:int = 8, masked = False) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = math.floor(d_model/num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.heads = [AttentionHead(head_dim=self.head_dim,masked=masked) for h in range(num_heads)]\n",
    "        self.linear = nn.Linear(d_model,d_model)\n",
    "    \n",
    "    def forward(self,X:torch.Tensor,Q:tuple,K:tuple,V:tuple) ->torch.Tensor:\n",
    "        heads_output = []\n",
    "        for head_index,head in enumerate(self.heads):\n",
    "                queries = Q[head_index]\n",
    "                keys = K[head_index]\n",
    "                values = V[head_index]\n",
    "                x = head(queries,keys,values) # this could be distributed to multiple devices for // processing\n",
    "                heads_output.append(x) # accumulate result\n",
    "        \n",
    "        o = torch.cat(heads_output,dim=-1)\n",
    "        linear_output = self.linear(o)\n",
    "        return self.layer_norm(X+linear_output)\n",
    "\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model=512,num_heads=8,masked=True)\n",
    "sample_data = torch.randn((5,10,512))\n",
    "splits = torch.split(sample_data,64,dim=2)\n",
    "mhsa(sample_data,splits,splits,splits).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model:int=512,d_ff:int=2048) -> None:\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model,out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=d_ff,out_features=d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_norm(X+self.ffn(X))\n",
    "\n",
    "\n",
    "# feedforward = FeedForward()\n",
    "# feedforward.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,d_ff:int,num_heads:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.feedforward = FeedForward(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        splits = torch.split(X,self.head_dim,dim=2)\n",
    "        return self.feedforward(self.multihead_self_attention(X,splits,splits,splits))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, number_of_encoder_blocks:int=6,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.encode = nn.Sequential(*[EncoderLayer(**kwargs) for n in range(number_of_encoder_blocks)])\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        return self.encode(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,d_ff:int,num_heads:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.masked_multi_head_self_attention = MultiHeadSelfAttention(d_model=d_model,num_heads=num_heads,masked=True)\n",
    "        self.multi_head_self_attention = MultiHeadSelfAttention(d_model=d_model,num_heads=num_heads,masked=False)\n",
    "        self.feedforward = FeedForward(d_model=d_model,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encoder sequence torch.Size([64, 10, 512])\n",
    "        \"\"\"\n",
    "\n",
    "        output_splits = torch.split(outputs,self.head_dim,dim=2)\n",
    "        encoded_sequence_splits = torch.split(encoded_sequence,self.head_dim,dim=2)\n",
    "        masked_output = self.masked_multi_head_self_attention(outputs,output_splits,output_splits,output_splits)\n",
    "        mhsa_output = self.multi_head_self_attention(masked_output,Q=masked_output,K=encoded_sequence_splits,V=encoded_sequence_splits)\n",
    "        \n",
    "        return self.feedforward(mhsa_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, number_of_decoder_blocks:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(**kwargs) for n in range(number_of_decoder_blocks)])\n",
    "        \n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor) -> torch.Tensor:\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            outputs = decoder_layer(outputs,encoded_sequence)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size:int,\n",
    "                 batch_size:int,\n",
    "                 context_size:int,\n",
    "                 d_model:int,\n",
    "                 d_ff:int,\n",
    "                 num_heads:int,\n",
    "                 number_of_encoder_blocks:int,\n",
    "                 number_of_decoder_blocks:int,\n",
    "                 p_drop:float):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.embedding = EmeddingsLayer(d_model=d_model,vocab_size=vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model,context_size=context_size)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.encoder = Encoder(\n",
    "                            vocab_size=batch_size,\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_encoder_blocks=number_of_encoder_blocks,\n",
    "                            p_drop=p_drop)\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "                            vocab_size=batch_size,\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            p_drop=p_drop,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_decoder_blocks=number_of_decoder_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=d_model,out_features=vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,X:torch.Tensor,y:torch.Tensor) -> torch.Tensor:\n",
    "        pos_encoding = self.positional_encoding()\n",
    "\n",
    "        input_embeddings = self.embedding(X) \n",
    "        inputs = self.dropout(input_embeddings+pos_encoding) # B*T*C\n",
    "        \n",
    "        encoded_sequence = self.encoder(inputs)\n",
    "        output_embedding = self.embedding(y)\n",
    "        outputs = self.dropout(output_embedding+pos_encoding) # B*T*C\n",
    "\n",
    "        decoder_output = self.decoder(outputs,encoded_sequence)\n",
    "\n",
    "        output_logits = self.linear(decoder_output)\n",
    "        output_probs = torch.softmax(output_logits,dim=-1)\n",
    "\n",
    "\n",
    "        return output_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 37000])\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vocab_size\":37000,\n",
    "    \"batch_size\":64,\n",
    "    \"context_size\":10,\n",
    "    \"d_model\":512,\n",
    "    \"num_heads\":1,\n",
    "    \"d_ff\":2048,\n",
    "    \"number_of_encoder_blocks\": 6,\n",
    "    \"number_of_decoder_blocks\": 6,\n",
    "    \"p_drop\":0.1\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Transformer(**config)\n",
    "\n",
    "X = torch.randint(1,config[\"vocab_size\"],(config[\"batch_size\"],config[\"context_size\"]))\n",
    "y = torch.randint(1,config[\"vocab_size\"],(config[\"batch_size\"],config[\"context_size\"]))\n",
    "\n",
    "o = model(X,y)\n",
    "print(o.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
