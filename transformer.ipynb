{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "Follows the \"Attention is all you need\" paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmeddingsLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.embedding(X) * math.sqrt(self.d_model) # \"In the embedding layers, we multiply those weights by sqrt(d_model)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Ecodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, context_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_size = context_size\n",
    "        # print(self.context_size)\n",
    "\n",
    "        self.pe = torch.zeros(self.context_size, self.d_model,requires_grad=False)\n",
    "        for pos in range(self.context_size):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.d_model)))\n",
    "                self.pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.d_model)))\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.pe.unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test embedding and position encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024])\n",
      "torch.Size([64, 1024, 512]) tensor([ 31.3549,  33.5606,  -1.3364,  -4.4340, -25.4117, -25.1762,  13.1974,\n",
      "        -42.2072, -12.0817,  -2.3737], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1024, 512]) tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "torch.Size([64, 1024, 512]) tensor([ 31.3549,  34.5606,  -1.3364,  -3.4340, -25.4117, -24.1762,  13.1974,\n",
      "        -41.2072, -12.0817,  -1.3737], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_test = EmeddingsLayer(d_model=512,vocab_size=50000)\n",
    "pencoding = PositionalEncoding(d_model=512,context_size=1024)\n",
    "example_data = torch.randint(1,50000,(64,1024))\n",
    "print(example_data.shape)\n",
    "embed_output = embed_test(example_data)\n",
    "print(embed_output.shape,embed_output[0][0][:10])\n",
    "pe_output = pencoding()\n",
    "print(pe_output.shape,pe_output[0][0][:10])\n",
    "embed_pos_output = embed_output + pe_output\n",
    "print(embed_pos_output.shape,embed_pos_output[0][0][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_dim:int,p_drop:float) -> None:\n",
    "        super().__init__()\n",
    "        self.queries= nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.keys = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.values = nn.Linear(in_features=head_dim,out_features=head_dim) # kaparthy set bias=False why?\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "\n",
    "    def forward(self,Q:torch.Tensor,K:torch.Tensor,V:torch.Tensor,mask:torch.Tensor = None) -> torch.Tensor:\n",
    "        B,T,C = K.shape\n",
    "        Q = self.dropout(self.queries(Q))\n",
    "        K = self.dropout(self.keys(K))\n",
    "        V = self.dropout(self.values(V))\n",
    "\n",
    "        scaled_dot_product_attention = (Q @ K.transpose(2,1))/torch.sqrt(torch.tensor(C))\n",
    "        if mask is not None:\n",
    "            scaled_dot_product_attention = scaled_dot_product_attention + mask\n",
    "            # print(scaled_dot_product_attention)\n",
    "\n",
    "        dot_product_softened = torch.softmax(scaled_dot_product_attention,dim=-1)\n",
    "        # print(dot_product_softened)\n",
    "        return dot_product_softened @ V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 512])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model:int, p_drop:float, num_heads:int = 8) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = math.floor(d_model/num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.heads = [AttentionHead(head_dim=self.head_dim,p_drop=p_drop) for h in range(num_heads)]\n",
    "        self.linear = nn.Linear(d_model,d_model)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "    \n",
    "    def forward(self,X:torch.Tensor,Q:tuple,K:tuple,V:tuple, mask:torch.Tensor = None) ->torch.Tensor:\n",
    "        heads_output = []\n",
    "        for head_index,head in enumerate(self.heads):\n",
    "                queries = Q[head_index]\n",
    "                keys = K[head_index]\n",
    "                values = V[head_index]\n",
    "                v = head(queries,keys,values,mask) # this could be distributed to multiple devices for // processing\n",
    "                heads_output.append(v) # accumulate result\n",
    "         \n",
    "        o = torch.cat(heads_output,dim=-1) #concat\n",
    "        linear_output = self.linear(o) #linear\n",
    "        dropped_output = self.dropout(linear_output) #dropout\n",
    "        return self.layer_norm(X+dropped_output)\n",
    "\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model=512,p_drop=0.1,num_heads=8)\n",
    "sample_data = torch.randn((5,10,512))\n",
    "splits = torch.split(sample_data,64,dim=2)\n",
    "mhsa(sample_data,splits,splits,splits,mask=None).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model:int,p_drop:float,d_ff:int) -> None:\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model,out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "            nn.Linear(in_features=d_ff,out_features=d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self,X:torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_norm(X+self.ffn(X))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model:int, p_drop:float, d_ff:int, num_heads:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.multihead_self_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads)\n",
    "        self.feedforward = FeedForward(d_model=d_model,p_drop=p_drop,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,X:torch.Tensor,mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        splits = torch.split(X,self.head_dim,dim=2)\n",
    "        return self.feedforward(self.multihead_self_attention(X,splits,splits,splits,mask))\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, number_of_encoder_blocks:int=6,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(**kwargs) for n in range(number_of_encoder_blocks)])\n",
    "\n",
    "    def forward(self,X:torch.Tensor,mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        outputs = X\n",
    "        for encoder_layer in self.encoders:\n",
    "            outputs = encoder_layer(outputs,mask)\n",
    "            # print(outputs[:5])\n",
    "            # print(\"\\n end of encoder layer \\n\")\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model:int,p_drop:float,d_ff:int,num_heads:int,**kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.masked_multi_head_self_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads)\n",
    "        self.masked_multi_head_cross_attention = MultiHeadSelfAttention(d_model=d_model,p_drop=p_drop,num_heads=num_heads)\n",
    "        self.feedforward = FeedForward(d_model=d_model,p_drop=p_drop,d_ff=d_ff)\n",
    "\n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor,self_attention_mask:torch.Tensor=None,cross_attention_mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        output_splits = torch.split(outputs,self.head_dim,dim=2)\n",
    "        encoded_sequence_splits = torch.split(encoded_sequence,self.head_dim,dim=2)\n",
    "        \n",
    "        masked_output = self.masked_multi_head_self_attention(outputs,Q=output_splits,K=output_splits,V=output_splits,mask=self_attention_mask)\n",
    "        mhsa_output = self.masked_multi_head_cross_attention(masked_output,Q=masked_output,K=encoded_sequence_splits,V=encoded_sequence_splits,mask=cross_attention_mask)\n",
    "        \n",
    "        return self.feedforward(mhsa_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, number_of_decoder_blocks:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(**kwargs) for n in range(number_of_decoder_blocks)])\n",
    "        \n",
    "    def forward(self,outputs:torch.Tensor,encoded_sequence:torch.Tensor,self_attention_mask:torch.Tensor=None,cross_attention_mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            outputs = decoder_layer(outputs,encoded_sequence,self_attention_mask,cross_attention_mask)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 eng_vocab_size:int,\n",
    "                 swa_vocab_size:int,\n",
    "                 batch_size:int,\n",
    "                 context_size:int,\n",
    "                 d_model:int,\n",
    "                 d_ff:int,\n",
    "                 num_heads:int,\n",
    "                 number_of_encoder_blocks:int,\n",
    "                 number_of_decoder_blocks:int,\n",
    "                 p_drop:float):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.encoder_embedding = EmeddingsLayer(d_model=d_model,vocab_size=eng_vocab_size)\n",
    "        self.decoder_embedding = EmeddingsLayer(d_model=d_model,vocab_size=swa_vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model,context_size=context_size)\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "        self.encoder = Encoder(\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_encoder_blocks=number_of_encoder_blocks,\n",
    "                            p_drop=p_drop)\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "                            batch_size=batch_size,\n",
    "                            context_size=context_size,\n",
    "                            d_model=d_model,\n",
    "                            p_drop=p_drop,\n",
    "                            d_ff=d_ff,\n",
    "                            num_heads=num_heads,\n",
    "                            number_of_decoder_blocks=number_of_decoder_blocks)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=d_model,out_features=swa_vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self,X:torch.Tensor,y:torch.Tensor,encoder_mask:torch.Tensor,decoder_self_attention_mask:torch.Tensor,decoder_cross_attention_mask:torch.Tensor) -> torch.Tensor:\n",
    "        pos_encoding = self.positional_encoding() #\n",
    "        # print(pos_encoding[:5])\n",
    "\n",
    "        # encode\n",
    "        input_embeddings = self.encoder_embedding(X) \n",
    "        # print(input_embeddings[:5])\n",
    "\n",
    "        inputs = self.dropout(input_embeddings+pos_encoding) # B*T*C\n",
    "        # print(inputs[:5])\n",
    "        # print(encoder_mask[:5])\n",
    "\n",
    "        encoded_sequence = self.encoder(inputs,encoder_mask)\n",
    "        # print(encoded_sequence[:5])\n",
    "        # decode\n",
    "        output_embedding = self.decoder_embedding(y)\n",
    "        outputs = self.dropout(output_embedding+pos_encoding) # B*T*C\n",
    "        decoder_output = self.decoder(outputs,encoded_sequence,self_attention_mask=decoder_self_attention_mask,cross_attention_mask=decoder_cross_attention_mask)\n",
    "        # linear\n",
    "        output_logits = self.linear(decoder_output)\n",
    "        # output_probs = torch.softmax(output_logits,dim=-1)\n",
    "\n",
    "\n",
    "        return output_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of swahili dataset: 5000 \n",
      "Size of english dataset: 5000 \n",
      "Max swahili sentence: 249 \n",
      "Max english sentence: 233 \n",
      "['Huyo ni rafiki yako mpya?', 'Job hana hamu ya mpira wa vikapu.', 'Adam aliniambia kuwa Alice alikuwa na mpenzi mpya wa kiume', 'Radio haikutanga kuhusu ajali hiyo.', 'Adamu ana wasiwasi tutapotea.']\n",
      "['Is that your new friend?', \"Jacob wasn't interested in baseball.\", 'Adam told me that Alice had a new boyfriend.', \"The radio didn't inform about the accident.\", \"Adam is worried we'll get lost.\"]\n",
      "Eng vocab: ['>', '\"', 'G', '9', 'é', 'F', '8', 't', 'x', 'n', ')', 'i', 'J', 'P', 'C', 'Q', '0', 'u', '&', 'j', '?', ' ', 'I', 's', 'q', 'c', '’', 'T', \"'\", 'O', ',', 'r', 'h', ';', 'p', 'R', '4', 'f', 'L', 'E', 'W', 'w', '$', '°', '5', 'à', 'S', '”', 'H', '6', 'U', 'v', '_', 'M', 'N', '.', '“', 'z', 'g', 'b', 'y', '(', '1', '!', '3', 'B', 'k', 'l', 'K', 'V', '—', 'o', 'd', '-', 'm', 'A', '7', 'e', 'Y', 'Z', 'a', 'D', ':', '2', '*', '<']\n",
      "Swa vocab: ['>', '\"', 'G', '9', 'F', '8', 't', 'x', 'n', ')', 'i', 'J', 'P', 'C', 'Q', '0', 'u', '&', 'j', '?', ' ', 'I', 's', 'q', 'c', 'T', \"'\", 'O', ',', 'r', 'h', ';', 'p', 'R', '4', 'f', 'L', 'E', '\\u200b', '+', 'W', 'w', '$', '5', 'S', '”', 'H', '6', 'U', 'v', 'M', 'N', '.', 'z', 'g', 'b', 'y', '(', '1', '!', '3', 'B', 'k', 'l', 'K', 'V', '—', 'o', 'd', '-', 'm', 'A', '7', 'e', 'Y', 'Z', 'a', 'D', ':', '2', '/', '*', '<']\n",
      "Eng vocab_size :83\n",
      "Swa vocab_size :86\n",
      "{'>': 0, '\"': 1, 'G': 2, '9': 3, 'F': 4, '8': 5, 't': 6, 'x': 7, 'n': 8, ')': 9, 'i': 10, 'J': 11, 'P': 12, 'C': 13, 'Q': 14, '0': 15, 'u': 16, '&': 17, 'j': 18, '?': 19, ' ': 20, 'I': 21, 's': 22, 'q': 23, 'c': 24, 'T': 25, \"'\": 26, 'O': 27, ',': 28, 'r': 29, 'h': 30, ';': 31, 'p': 32, 'R': 33, '4': 34, 'f': 35, 'L': 36, 'E': 37, '\\u200b': 38, '+': 39, 'W': 40, 'w': 41, '$': 42, '5': 43, 'S': 44, '”': 45, 'H': 46, '6': 47, 'U': 48, 'v': 49, 'M': 50, 'N': 51, '.': 52, 'z': 53, 'g': 54, 'b': 55, 'y': 56, '(': 57, '1': 58, '!': 59, '3': 60, 'B': 61, 'k': 62, 'l': 63, 'K': 64, 'V': 65, '—': 66, 'o': 67, 'd': 68, '-': 69, 'm': 70, 'A': 71, '7': 72, 'e': 73, 'Y': 74, 'Z': 75, 'a': 76, 'D': 77, ':': 78, '2': 79, '/': 80, '*': 81, '<': 82}\n",
      "{'>': 0, '\"': 1, 'G': 2, '9': 3, 'é': 4, 'F': 5, '8': 6, 't': 7, 'x': 8, 'n': 9, ')': 10, 'i': 11, 'J': 12, 'P': 13, 'C': 14, 'Q': 15, '0': 16, 'u': 17, '&': 18, 'j': 19, '?': 20, ' ': 21, 'I': 22, 's': 23, 'q': 24, 'c': 25, '’': 26, 'T': 27, \"'\": 28, 'O': 29, ',': 30, 'r': 31, 'h': 32, ';': 33, 'p': 34, 'R': 35, '4': 36, 'f': 37, 'L': 38, 'E': 39, 'W': 40, 'w': 41, '$': 42, '°': 43, '5': 44, 'à': 45, 'S': 46, '”': 47, 'H': 48, '6': 49, 'U': 50, 'v': 51, '_': 52, 'M': 53, 'N': 54, '.': 55, '“': 56, 'z': 57, 'g': 58, 'b': 59, 'y': 60, '(': 61, '1': 62, '!': 63, '3': 64, 'B': 65, 'k': 66, 'l': 67, 'K': 68, 'V': 69, '—': 70, 'o': 71, 'd': 72, '-': 73, 'm': 74, 'A': 75, '7': 76, 'e': 77, 'Y': 78, 'Z': 79, 'a': 80, 'D': 81, ':': 82, '2': 83, '*': 84, '<': 85}\n"
     ]
    }
   ],
   "source": [
    "# read in the data from files to lists of strings\n",
    "swa_sentences = []\n",
    "with open(\"./data/translate/gamayun_kit5k.swa\",\"r\") as f:\n",
    "    swa_sentences = f.readlines()\n",
    "eng_sentences = []\n",
    "with open(\"./data/translate/gamayun_kit5k.eng\",\"r\") as f:\n",
    "    eng_sentences = f.readlines()\n",
    "\n",
    "# remove last \\n at the end of every line\n",
    "swa_sentences = [s.rstrip(\"\\n\") for s in swa_sentences]\n",
    "eng_sentences = [s.rstrip(\"\\n\") for s in eng_sentences]\n",
    "\n",
    "print(f\"Size of swahili dataset: {len(swa_sentences)} \")\n",
    "print(f\"Size of english dataset: {len(eng_sentences)} \")\n",
    "print(f\"Max swahili sentence: {max([len(s) for s in swa_sentences])} \")\n",
    "print(f\"Max english sentence: {max([len(s) for s in eng_sentences])} \")\n",
    "\n",
    "print(swa_sentences[:5])\n",
    "print(eng_sentences[:5])\n",
    "\n",
    "START_TOKEN = '>'\n",
    "PADDING_TOKEN = '*'\n",
    "END_TOKEN = '<'\n",
    "\n",
    "# prep swa vocab\n",
    "swa_vocab = list(set(''.join(swa_sentences)))\n",
    "swa_vocab.insert(0,START_TOKEN)\n",
    "swa_vocab.append(PADDING_TOKEN)\n",
    "swa_vocab.append(END_TOKEN)\n",
    "\n",
    "# prep eng vocab\n",
    "eng_vocab = list(set(''.join(eng_sentences)))\n",
    "eng_vocab.insert(0,START_TOKEN)\n",
    "eng_vocab.append(PADDING_TOKEN)\n",
    "eng_vocab.append(END_TOKEN)\n",
    "\n",
    "print(f\"Eng vocab: {eng_vocab}\")\n",
    "print(f\"Swa vocab: {swa_vocab}\")\n",
    "\n",
    "swa_vocab_size = len(swa_vocab)\n",
    "eng_vocab_size = len(eng_vocab)\n",
    "\n",
    "\n",
    "print(f\"Eng vocab_size :{len(swa_vocab)}\")\n",
    "print(f\"Swa vocab_size :{len(eng_vocab)}\")\n",
    "\n",
    "swa_token_to_index = {t:i for i,t in enumerate(swa_vocab)}\n",
    "print(swa_token_to_index)\n",
    "swa_index_to_token = {i:t for i,t in enumerate(swa_vocab)}\n",
    "# print(swa_index_to_token)\n",
    "eng_token_to_index = {t:i for i,t in enumerate(eng_vocab)}\n",
    "print(eng_token_to_index)\n",
    "eng_index_to_token = {i:t for i,t in enumerate(eng_vocab)}\n",
    "# print(eng_index_to_token)\n",
    "\n",
    "# tokenize\n",
    "swahili_sentences_tokenized = [[swa_token_to_index[t] for t in s] for s in swa_sentences]\n",
    "english_sentences_tokenized = [[eng_token_to_index[t] for t in s] for s in eng_sentences]\n",
    "\n",
    "# train/test split\n",
    "swahili_sentences_tokenized_train = swahili_sentences_tokenized[:4500]\n",
    "swahili_sentences_tokenized_test = swahili_sentences_tokenized[4500:]\n",
    "english_sentences_tokenized_train = english_sentences_tokenized[:4500]\n",
    "english_sentences_tokenized_test = english_sentences_tokenized[4500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, swahili_sentences, english_sentences,transforms=None,eng_max_sequence_length = 250,swa_max_sequence_length = 250):\n",
    "        print(swa_max_sequence_length,eng_max_sequence_length)\n",
    "        self.english_sentences = english_sentences\n",
    "        self.swahili_sentences = swahili_sentences\n",
    "        self.transforms = transforms\n",
    "        self.eng_max_sequence_length = eng_max_sequence_length\n",
    "        self.swa_max_sequence_length = swa_max_sequence_length\n",
    "        self.encoder_padding_mask = torch.full([self.eng_max_sequence_length, self.eng_max_sequence_length] , False) # each sentence gets a mask\n",
    "        self.look_ahead_mask = torch.triu(torch.full([self.swa_max_sequence_length, self.swa_max_sequence_length] , True), diagonal=1)\n",
    "        self.decoder_padding_mask_self_attention = torch.full([self.swa_max_sequence_length, self.swa_max_sequence_length] , False) # each sentence gets a mask\n",
    "        self.decoder_padding_mask_cross_attention = torch.full([self.swa_max_sequence_length, self.eng_max_sequence_length] , False) # each sentence gets a mask\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng_sentence = self.english_sentences[idx]\n",
    "        swa_sentence = self.swahili_sentences[idx]\n",
    "        eng_sentence_length = len(eng_sentence)\n",
    "        swa_sentence_length = len(swa_sentence)\n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length, self.eng_max_sequence_length) # fillers\n",
    "        swa_chars_to_padding_mask = np.arange(swa_sentence_length, self.swa_max_sequence_length) # fillers\n",
    "\n",
    "        for _ in range(len(eng_sentence), self.eng_max_sequence_length):\n",
    "            eng_sentence.append(eng_token_to_index[PADDING_TOKEN])\n",
    "        for _ in range(len(swa_sentence), self.swa_max_sequence_length):\n",
    "            swa_sentence.append(swa_token_to_index[PADDING_TOKEN])\n",
    "\n",
    "        self.encoder_padding_mask[:, eng_chars_to_padding_mask] = True\n",
    "        self.encoder_padding_mask[eng_chars_to_padding_mask, :] = True\n",
    "        encoder_padding_mask = torch.where(self.encoder_padding_mask, NEG_INFTY, 0) # encoder mask\n",
    "\n",
    "        self.decoder_padding_mask_self_attention[:, swa_chars_to_padding_mask] = True\n",
    "        self.decoder_padding_mask_self_attention[swa_chars_to_padding_mask, :] = True\n",
    "        decoder_self_attention_mask = torch.where(self.decoder_padding_mask_self_attention+self.look_ahead_mask, NEG_INFTY, 0) # decoder self-attention mask\n",
    "\n",
    "        self.decoder_padding_mask_cross_attention[:, eng_chars_to_padding_mask] = True\n",
    "        self.decoder_padding_mask_cross_attention[swa_chars_to_padding_mask, :] = True\n",
    "        decoder_cross_attention_mask = torch.where(self.decoder_padding_mask_cross_attention, NEG_INFTY, 0) # decoder cross-attention mask\n",
    "\n",
    "        if self.transforms:\n",
    "            swa_sentence = self.transforms(swa_sentence)\n",
    "            eng_sentence = self.transforms(eng_sentence)\n",
    "\n",
    "        return eng_sentence,encoder_padding_mask, swa_sentence, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n",
      "250 250\n",
      "torch.Size([10, 250]) torch.Size([10, 250])\n",
      "tensor([27, 32, 11, 23, 21, 11, 23, 21,  7, 71, 71, 21, 58, 71, 71, 72, 21, 80,\n",
      "        21, 25, 32, 80,  9, 25, 77, 21,  7, 71, 21, 67, 71, 23, 77, 55, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "training_dataset = TranslationDataset(swahili_sentences=swahili_sentences_tokenized_train,english_sentences=english_sentences_tokenized_train,transforms=transforms.ToTensor())\n",
    "testing_dataset = TranslationDataset(swahili_sentences=swahili_sentences_tokenized_test,english_sentences=english_sentences_tokenized_test,transforms=transforms.ToTensor())\n",
    "training_dataloader = DataLoader(training_dataset,batch_size=10,shuffle=True)\n",
    "testing_dataloader = DataLoader(testing_dataset,batch_size=10,shuffle=False)\n",
    "\n",
    "eng_sentence,encoder_mask,swa_sentence,decoder_self_attention_mask,decoder_cross_attention_mask = next(iter(training_dataloader))\n",
    "print(eng_sentence.shape,swa_sentence.shape)\n",
    "print(eng_sentence[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"eng_vocab_size\":eng_vocab_size,\n",
    "    \"swa_vocab_size\":swa_vocab_size,\n",
    "    \"batch_size\":10,\n",
    "    \"context_size\":250,\n",
    "    \"d_model\":512,\n",
    "    \"num_heads\":1,\n",
    "    \"d_ff\":2048,\n",
    "    \"number_of_encoder_blocks\": 1,\n",
    "    \"number_of_decoder_blocks\": 1,\n",
    "    \"p_drop\":0.1\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Transformer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 0.0007934271707199514\n",
      "English: The driver really screwed up the team when he drove them to the wrong playing field in a different town.**************************************************************************************************************************************************\n",
      "Swahili Translation: Dereva aliikosea timu kweli wakati alipowapeleka kwenye uwanja usiofaa wa kucheza ulio mji tofauti.*******************************************************************************************************************************************************\n",
      "Swahili Prediction: *ere** alii*o**a **mu kw*li *a*ati ali*ow**eleka *wen*e *wan*a u*iofaa wa **cheza ulio mji *ofa***********************************************************************************************************************************************************\n",
      "Training Loss: 0.0007934271707199514\n",
      "Test English: My airport shuttle bus leaves at six o'clock.*************************************************************************************************************************************************************************************************************\n",
      "Test Swahili Prediction: *ere** alii*o**a **mu kw*li *a*ati ali*ow**eleka *wen*e *wan*a u*iofaa wa **cheza ulio mji *ofa*************************************************************************************************************************************************************** la*gu *utoka uwan*ani wa n******inaon*o*a**aa****i*na***i*i******************************************************************************************************************************************************************************************\n",
      "Testing Loss: 0.0004978062934242189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m valid_indicies \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(swa_batch\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m swa_token_to_index[PADDING_TOKEN], \u001b[39mFalse\u001b[39;00m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m valid_indicies\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> 14\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/training/pytorch_deeplearning/env/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/training/pytorch_deeplearning/env/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for batch,(eng_batch,encoder_mask,swa_batch,decoder_self_attention_mask,decoder_cross_attention_mask) in enumerate(training_dataloader):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_logits = model(eng_batch,swa_batch,encoder_mask,decoder_self_attention_mask,decoder_cross_attention_mask)\n",
    "        loss = loss_fn(train_logits.view(-1,swa_vocab_size),swa_batch.view(-1))\n",
    "        valid_indicies = torch.where(swa_batch.view(-1) == swa_token_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Iteration {batch} : {loss.item()}\")\n",
    "            print(f\"English: {''.join([eng_index_to_token[t.item()] for t in eng_batch[0]])}\")\n",
    "            print(f\"Swahili Translation: {''.join([swa_index_to_token[t.item()] for t in swa_batch[0]])}\")\n",
    "            swa_sentence_predicted = torch.argmax(train_logits[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in swa_sentence_predicted:\n",
    "              if idx == swa_token_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += swa_index_to_token[idx.item()]\n",
    "            print(f\"Swahili Prediction: {predicted_sentence}\")\n",
    "            print(f\"Training Loss: {loss.item()}\")\n",
    "\n",
    "            # test\n",
    "            model.eval()\n",
    "            for test_batch,(test_eng_batch,test_encoder_mask,test_swa_batch,test_decoder_self_attention_mask,test_decoder_cross_attention_mask) in enumerate(testing_dataloader):\n",
    "                print(f\"Test English: {''.join([eng_index_to_token[t.item()] for t in test_eng_batch[0]])}\")\n",
    "                test_logits = model(test_eng_batch,test_swa_batch,test_encoder_mask,test_decoder_self_attention_mask,test_decoder_cross_attention_mask)\n",
    "                swa_sentence_predicted = torch.argmax(test_logits[0], axis=1)\n",
    "                for idx in swa_sentence_predicted:\n",
    "                  if idx == swa_token_to_index[END_TOKEN]:\n",
    "                    break\n",
    "                  predicted_sentence += swa_index_to_token[idx.item()]\n",
    "                print(f\"Test Swahili Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "                loss = loss_fn(test_logits.view(-1,swa_vocab_size),test_swa_batch.view(-1))\n",
    "                valid_indicies = torch.where(test_swa_batch.view(-1) == swa_token_to_index[PADDING_TOKEN], False, True)\n",
    "                loss = loss.sum() / valid_indicies.sum()\n",
    "                print(f\"Testing Loss: {loss.item()}\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
